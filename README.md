## About

Eye movement recording is quickly becoming ubiquitous across different interfaces.  The ability to draw inferences about processing from eye movements has long been used in academic settings.  Here, we will explore how the connection between eye movements and cognitive processing, as related to cognitive state, can be used in applications.
The cognitive state of a user can inform not only whether or not they stay on task, but what type of thinking (explorative, expert, creative) is currently engaged. With the advent of AI in this field, eye movement recordings can be used to decode, guide and encourage different cognitive states. 

This workshop will be used as a space in which researchers from across different disciplines (HCI, psychology, AI, cognitive neuroscience) can interact and strengthen this budding field.  The use of eye movements to decode cognitive states could be extended to adaptive interfaces that could use eye movements as feedback to guide attention, processing, learning and memory. 
The development of such tools could have some far reaching implications for our society and launch a whole new way of human-computer interaction.

## Workshop Agenda
09:00 -- Welcome & Primer: [Monica Castelhano](https://qvcl.ca/) (Queens University) <br/>
09:20 -- Invited Speaker: [Andrew Duchowski](http://andrewd.ces.clemson.edu/) (Clemson University) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Eye tracking measures of cognitive load"* <br/>
09:45 -- Invited Speaker: [Laura Matzen](https://neuroscience.sandia.gov/people/Matzen.html) (Sandia National Laboratories) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; *"Using eye movements to assess human comprehension of data visualizations"* <br/>
10:10 -- Invited Speaker: [Oleg Komogortsev](https://userweb.cs.txstate.edu/~ok11/) (Texas State University) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Virtual and Augmented Reality, Sensors, Eye Movements, and Their Impact on Healthcare and Security"* <br/>
10:35 -- Coffee Break (10:45 Poster set-up) <br/>
11:00 -- Poster Session <br/>
12:05 -- Invited Speaker: [Enkelejda Kasneci](http://www.ti.uni-tuebingen.de/Prof-Dr-Enkelejda-Kasneci.956.0.html) (University of Tübingen) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Beyond Zero Order Metrics: Deep Learning based Scanpath Analytics for Cognitive State Recognition"* <br/>
12:30 -- Lunch Break <br/>
14:00 -- Invited Speaker: [Greg Zelinsky](https://you.stonybrook.edu/zelinsky/) (Stony Brook University) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Predicting goal-directed attention control using deep neural networks"* <br/>
14:25 -- Invited Speaker: [Alexandra Papoutsaki](http://www.cs.pomona.edu/~apapoutsaki/) (Pomona College) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Gaze sharing for remote collaboration"* <br/>
14:50 -- Invited Speaker: [Bonita Sharif](https://www.shbonita.me/) (University of Nebraska) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Eye tracking for reading and comprehension"* <br/>
15:15 -- Invited Speaker: [Mary Hayhoe](http://www.cs.utexas.edu/~dana/vrlab/) (University of Texas) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Eye movements and sensory-motor decisions in natural behavior"* <br/>
15:40 -- Coffee Break <br/>
16:00 -- Panel Discussion (all the invited speakers) <br/>
16:50 -- Closing Remarks: [Zoya Bylinskii](http://web.mit.edu/zoya/www/index.html) (Adobe Research) <br/>

## About the Speakers
[**Andrew Duchowski**](http://andrewd.ces.clemson.edu/) is a Professor in the School of Computing at Clemson University and his research and teaching interests include visual attention and perception, eye tracking, computer vision, and computer graphics.  

[**Laura Matzen**](https://neuroscience.sandia.gov/people/Matzen.html) is a Principal Member of the Technical Staff in the Cognitive Science and Systems department at Sandia National Laboratories. Her primary research interests lie in using cognitive neuroscience methods to understand how humans process and remember information while performing complex reasoning tasks.

[**Oleg Komogortsev**](https://userweb.cs.txstate.edu/~ok11/) is currently a Professor of Computer Science at Texas State University. He conducts research in eye tracking with a focus on health assessment, cyber security (biometrics), bioengineering, human computer interaction, and usability. 

[**Enkelejda Kasneci**](http://www.ti.uni-tuebingen.de/Prof-Dr-Enkelejda-Kasneci.956.0.html) is a Junior Professor of Computer Science at the University of Tübingen, and her main research interests are applied machine learning, eye-tracking technology and applications, and driver assistance systems.

[**Gregory Zelinsky**](https://you.stonybrook.edu/zelinsky/) is a Professor of Cognitive Science at Stony Brook University. His  research attempts to integrate cognitive, computational, and neuroimaging techniques to better understand a broad range of visual cognitive behaviors, including visual search, object representation, working memory, and scene perception.

[**Alexandra Papoutsaki**](http://www.cs.pomona.edu/~apapoutsaki/) is an Assistant Professor of Computer Science at Pomona College. Her main research focuses on eye tracking and gaze sharing in the context of remote collaboration. She received her PhD from Brown University where she developed a new approach for webcam eye tracking in the browser.

[**Bonita Sharif**](https://www.shbonita.me/) is an Assistant Professor in the Department of Computer Science and Engineering at University of Nebraska. Her research interests are in eye tracking related to software engineering, empirical software engineering, emotional awareness, software traceability, and software visualization to support maintenance of large systems.

[**Mary Hayhoe**](http://www.cs.utexas.edu/~dana/vrlab/) is a Professor of Psychology at University of Texas Austin. Her work has made a number of highly innovative and important contributions to our understanding of visual sensation, perception and cognition.

## Organizers
[**Monica Castelhano**](https://qvcl.ca/) is an Associate Professor in Psychology at Queen’s University. Dr. Castelhano studies memory and attentional processes in complex visual environments in real and virtual settings using techniques such as VR, eye tracking and electroencephalogram (EEG).  Dr. Castelhano has been granted numerous awards including the Early Researcher Award from OSF and numerous grants from national and international funding agencies. She is currently an Associate Editor at the Quarterly Journal of Experimental Psychology.

[**Zoya Bylinskii**](http://web.mit.edu/zoya/www/index.html) is a Research Scientist at Adobe Inc. She received a PhD in Electrical Engineering and Computer Science from MIT in September 2018 and an Hon. B.Sc. in Computer Science and Statistics from the University of Toronto in 2012. Zoya is a 2016 Adobe Research Fellow, a 2014-2016 NSERC Postgraduate Scholar, a 2013 Julie Payette Research Scholar, and a 2011 Anita Borg Scholar. Zoya works at the interface of human vision, computer vision, and human-computer interaction: building computational models of people's memory and attention, and applying the findings to graphic designs and data visualizations.

[**Xi Wang**](http://cybertron.cg.tu-berlin.de/xiwang/) is a graduate student at the Technische Universität Berlin. She has an M.Eng. degree from Shanghai Jiao Tong University and an M.Sc. degree in computer science from Technische Universität Berlin. Her research is broadly concerned with human perception and its applications in computer graphics. She has worked on projects studying vergence eye movements in 3D space, using mental imagery paradigm to study encoded visual content in (episodic) memory and measuring where humans look on real three-dimensional stimuli.  

[**James Hillis**](https://www.linkedin.com/in/jameshillis/) is a Research Scientist at Facebook Reality Labs. He completed a Vision Science PhD in sensory cue combination at UC Berkeley in 2002 and post-doctoral research in color vision at the University of Pennsylvania before joining the faculty at the University of Glasgow in 2006. At the University of Glasgow he headed a lab studying socio-economic decision making and worked on an ESRC funded project on the cognitive neuroscience of social decision making. In 2013, he became a research scientist in the optical and display system division of 3M and in 2016 he joined Oculus Research which then became part of Facebook Reality Labs. James’ research focuses on the development of computational models of interactions between humans and their social and physical environments.


## Call for Participation

In this workshop we will explore how the connection between eye movements and cognitive processing can be used for applications. We invite papers on topics related to the use of eye movements (fixations, saccades, blinks, pupil dilations) for the analysis or prediction of cognitive processing.  This could include, but is not limited to, a viewer’s cognitive load or affective state, expertise or individual traits, the task type or difficulty, as well as their relationship to user interfaces broadly construed (driving, reading, problem solving, navigating, data analysis, coding, creating content, etc.). 

We invite submissions of a maximum of 4 pages, excluding references. The length of the submission should be commensurate with the contribution. Submissions will be sent out for review to 2 experts, who will rate the quality of the submission as well as the topic fit to the workshop. Submissions should follow the [ACM SIGCHI Extended Abstracts format](http://chi2019.acm.org/authors/chi-proceedings-format/). Information about where and when to submit the abstracts will be posted on https://emics-2020.github.io/CHI/. At least one author of each accepted paper must attend the workshop and all participants must register for both the workshop and for at least one day of the conference. 

Accepted papers will be presented as posters at the workshop. 

## Related Papers

E. Abdulin, **O. Komogortsev**. [User eye fatigue detection via eye movement behavior.](https://userweb.cs.txstate.edu/~ok11/papers_published/2015_CHI_Ab_Ko.pdf) CHI'EA (2015).

**A. Duchowski**, K. Krejtz, I. Krejtz, C. Biele, A. Niedzielska, P. Kiefer, M. Raubal, I. Giannopoulos. [The index of pupillary activity: measuring cognitive load vis-à-vis task difficulty with pupil oscillation.](https://par.nsf.gov/servlets/purl/10096514) CHI (2018).

C. Holland, **O. Komogortsev**, D. Tamir. [Identifying usability issues via algorithmic detection of excessive visual search.](https://www.researchgate.net/profile/Corey_Holland/publication/254005203_Identifying_Usability_Issues_via_Algorithmic_Detection_of_Excessive_Visual_Search/links/5712b44008ae4ef7452619d4.pdf) CHI (2012).

R. Gergely, **A. Duchowski**, M. Pohl. [Designing online tests for a virtual learning environment - evaluation of visual behavior between tasks.](https://publik.tuwien.ac.at/files/PubDat_232487.pdf) International Conference on Human Behavior in Design (2014).

P. Kiefer, I. Giannopoulos, M. Raubal, **A. Duchowski**. [Eye tracking for spatial research: Cognition, computation, challenges.](https://n.ethz.ch/~pekiefer/download/papers/kiefer2017scc_preprint.pdf) Spatial Cognition & Computation (2017)

D. Lohr, S.-H. Berndt, **O. Komogortsev**. [An implementation of eye movement-driven biometrics in virtual reality.](https://par.nsf.gov/servlets/purl/10074023) ETRA (2018).

**L. Matzen**, M. Haass, K. Divis, M. Stites. [Patterns of attention: How data visualizations are read.](https://www.osti.gov/servlets/purl/1425321) International Conference on Augmented Cognition (2017).

S. Vitak, J. Ingram, **A. Duchowski**, S. Ellis, A. Gramopadhye. [Gaze-augmented think-aloud as an aid to learning](https://dl.acm.org/citation.cfm?id=2208710) CHI (2012).

T Appel, C. Scharinger, P. Gerjets, **E. Kasneci**. [Cross-subject workload classification using pupil-related measures](https://dl.acm.org/citation.cfm?id=3204531) ETRA (2018).

**E. Kasneci**, G. Kasneci, T. Kübler, W. Rosenstiel. [Online recognition of fixations, saccades, and smooth pursuits for automated analysis of traffic hazard perception.](http://ti.uni-tuebingen.de/uploads/tx_timitarbeiter/Kasneci_et_al_Online_Recognition_Eye_Movements.pdf) Artificial neural networks (2015).

N. Castner, **E. Kasneci**, T. Kübler, K. Scheiter, J. Richter, T. Eder, F. Hüttig, C. Keutel. [Scanpath comparison in medical image reading skills of dental students: distinguishing stages of expertise development.](https://dl.acm.org/citation.cfm?id=3204550) ETRA (2018).

## Partners
<img height="100" src="https://emics-2020.github.io/EMICS/assets/logos/Adobe.png">
<img height="100" src="https://emics-2020.github.io/EMICS/assets/logos/frl.png">

