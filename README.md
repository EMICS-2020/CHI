## About

Eye movement recording is quickly becoming ubiquitous across different interfaces.  The ability to draw inferences about processing from eye movements has long been used in academic settings.  Here, we will explore how the connection between eye movements and cognitive processing, as related to cognitive state, can be used in applications.
The cognitive state of a user can inform not only whether or not they stay on task, but what type of thinking (explorative, expert, creative) is currently engaged. With the advent of A.I. in this field, eye movement recordings can be used to decode, guide and encourage different cognitive states. 

This SIG meeting will be used as a space in which researchers from across different disciplines (HCI, psychology, A.I., cognitive neuroscience) can interact and strengthen this budding field.  The use of eye movements to decode cognitive states could be extended to adaptive interfaces that could use eye movements as feedback to guide attention, processing, learning and memory. 
The development of such tools could have some far reaching implications for our society and launch a whole new way of human-computer interaction.

## SIG Meeting Agenda
To facilitate discussion, we will kick off the SIG with a panel of experts giving 5-7 minute lightning talks about applications of eye movements as an interface to cognitive state. 

1. Welcome & Primer: [Monica Castelhano](https://qvcl.ca/) (Queen's University) <br/>
2. Lighting Talk by Invited Panelist: [Andrew Duchowski](http://andrewd.ces.clemson.edu/) (Clemson University) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Eye Tracking Measures of Cognitive Load"* <br/>
3. Lighting Talk by Invited Panelist: [Alexandra Papoutsaki](http://www.cs.pomona.edu/~apapoutsaki/) (Pomona College) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Gaze Sharing for Remote Collaboration"* <br/>
4. Lighting Talk by Invited Panelist: [Oleg Komogortsev](https://userweb.cs.txstate.edu/~ok11/) (Texas State University) <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;*"Virtual and Augmented Reality, Sensors, Eye Movements, and Their Impact on Healthcare and Security"* <br/>
5. Panel Discussion (all the invited panelists) <br/>
6. Closing Remarks: [Zoya Bylinskii](http://web.mit.edu/zoya/www/index.html) (Adobe Research) <br/>

## About the Panelists
[**Andrew Duchowski**](http://andrewd.ces.clemson.edu/) is a Professor in the School of Computing at Clemson University and his research and teaching interests include visual attention and perception, eye tracking, computer vision, and computer graphics.  

[**Alexandra Papoutsaki**](http://www.cs.pomona.edu/~apapoutsaki/) is an Assistant Professor of Computer Science at Pomona College. Her main research focuses on eye tracking and gaze sharing in the context of remote collaboration. She received her PhD from Brown University where she developed a new approach for webcam eye tracking in the browser.

[**Oleg Komogortsev**](https://userweb.cs.txstate.edu/~ok11/) is currently a Professor of Computer Science at Texas State University. He conducts research in eye tracking with a focus on health assessment, cyber security (biometrics), bioengineering, human computer interaction, and usability. 

## Organizers
[**Monica Castelhano**](https://qvcl.ca/) is an Associate Professor in Psychology at Queen’s University. Dr. Castelhano studies memory and attentional processes in complex visual environments in real and virtual settings using techniques such as VR, eye tracking and electroencephalogram (EEG).  Dr. Castelhano has been granted numerous awards including the Early Researcher Award from OSF and numerous grants from national and international funding agencies. She is currently an Associate Editor at the Quarterly Journal of Experimental Psychology.

[**Zoya Bylinskii**](http://web.mit.edu/zoya/www/index.html) is a Research Scientist at Adobe Inc. She received a PhD in Electrical Engineering and Computer Science from MIT in September 2018 and an Hon. B.Sc. in Computer Science and Statistics from the University of Toronto in 2012. Zoya is a 2016 Adobe Research Fellow, a 2014-2016 NSERC Postgraduate Scholar, a 2013 Julie Payette Research Scholar, and a 2011 Anita Borg Scholar. Zoya works at the interface of human vision, computer vision, and human-computer interaction: building computational models of people's memory and attention, and applying the findings to graphic designs and data visualizations.

[**Xi Wang**](http://cybertron.cg.tu-berlin.de/xiwang/) is a graduate student at the Technische Universität Berlin. She has an M.Eng. degree from Shanghai Jiao Tong University and an M.Sc. degree in computer science from Technische Universität Berlin. Her research is broadly concerned with human perception and its applications in computer graphics. She has worked on projects studying vergence eye movements in 3D space, using mental imagery paradigm to study encoded visual content in (episodic) memory and measuring where humans look on real three-dimensional stimuli.  

[**James Hillis**](https://www.linkedin.com/in/jameshillis/) is a Research Scientist at Facebook Reality Labs. He completed a Vision Science PhD in sensory cue combination at UC Berkeley in 2002 and post-doctoral research in color vision at the University of Pennsylvania before joining the faculty at the University of Glasgow in 2006. At the University of Glasgow he headed a lab studying socio-economic decision making and worked on an ESRC funded project on the cognitive neuroscience of social decision making. In 2013, he became a research scientist in the optical and display system division of 3M and in 2016 he joined Oculus Research which then became part of Facebook Reality Labs. James’ research focuses on the development of computational models of interactions between humans and their social and physical environments.

[**Andrew Duchowski**](http://andrewd.ces.clemson.edu/) is a professor of Computer Science at Clemson University. He received his baccalaureate (1990) from Simon Fraser University, Burnaby, Canada, and doctorate (1997) from Texas A&M University, College Station, TX, both in Computer Science. His research and teaching interests include visual attention and perception, eye tracking, computer vision, and computer graphics. He is a noted research leader in the field of eye tracking, having produced a corpus of papers and a monograph related to eye tracking research, and has delivered courses and seminars on the subject at international conferences. He maintains Clemson's eye tracking laboratory, and teaches a regular course on eye tracking methodology attracting students from a variety of disciplines across campus.

## Related Papers

E. Abdulin, **O. Komogortsev**. [User eye fatigue detection via eye movement behavior.](https://userweb.cs.txstate.edu/~ok11/papers_published/2015_CHI_Ab_Ko.pdf) CHI'EA (2015).

J. Jacobs, **X. Wang**, M. Alexa. [Keep it simple: Depth-based dynamic adjustment of rendering for head-mounted displays decreases visual comfort.](https://dl.acm.org/citation.cfm?id=3353902) TAP (2019).

N. Abid, J. Maletic, **B. Sharif**. [Using developer eye movements to externalize the mental model used in code summarization tasks.](http://www.cs.kent.edu/~jmaletic/papers/ETRA19full.pdf) ETRA (2019).

T. Appel, C. Scharinger, P. Gerjets, **E. Kasneci**. [Cross-subject workload classification using pupil-related measures.](https://dl.acm.org/citation.cfm?id=3204531) ETRA (2018).

D. Ballard, **M. Hayhoe**. [Modelling the role of task in the control of gaze.](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2856937/) Visual cognition (2009).

S. Brennan, X. Chen, C. A Dickinson, M. Neider, **G. Zelinsky**. [Coordinating cognition: The costs and benefits of shared gaze during collaborative search.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.298.4432&rep=rep1&type=pdf) Cognition (2008).

**Z. Bylinskii**, M. Borkin, N. Kim, H. Pfister, A. Oliva. [Eye fixation metrics for large scale evaluation and comparison of information visualizations.](https://pdfs.semanticscholar.org/6708/9163a209c68318f17b25d744d680d73a7f80.pdf) Eye Tracking and Visualization (2017).

**Z. Bylinskii**, N. Kim, P. O'Donovan, S. Alsheikh, S. Madan, H. Pfister, F. Durand, B. Russell, A. Hertzmann. [Learning visual importance for graphic designs and data visualizations.](https://dl.acm.org/citation.cfm?id=3126653) UIST (2017).

**M. Castelhano**, M. Mack, J. Henderson. [Viewing task influences eye movement control during active scene perception.](https://iovs.arvojournals.org/article.aspx?articleid=2193534) Journal of vision (2009).

N. Castner, **E. Kasneci**, T. Kübler, K. Scheiter, J. Richter, T. Eder, F. Hüttig, C. Keutel. [Scanpath comparison in medical image reading skills of dental students: distinguishing stages of expertise development.](https://dl.acm.org/citation.cfm?id=3204550) ETRA (2018).

**A. Duchowski**, K. Krejtz, I. Krejtz, C. Biele, A. Niedzielska, P. Kiefer, M. Raubal, I. Giannopoulos. [The index of pupillary activity: measuring cognitive load vis-à-vis task difficulty with pupil oscillation.](https://par.nsf.gov/servlets/purl/10096514) CHI (2018).

C. Holland, **O. Komogortsev**, D. Tamir. [Identifying usability issues via algorithmic detection of excessive visual search.](https://www.researchgate.net/profile/Corey_Holland/publication/254005203_Identifying_Usability_Issues_via_Algorithmic_Detection_of_Excessive_Visual_Search/links/5712b44008ae4ef7452619d4.pdf) CHI (2012).

R. Gergely, **A. Duchowski**, M. Pohl. [Designing online tests for a virtual learning environment - evaluation of visual behavior between tasks.](https://publik.tuwien.ac.at/files/PubDat_232487.pdf) International Conference on Human Behavior in Design (2014).

**M. Hayhoe**. [Advances in relating eye movements and cognition.](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.501.7665&rep=rep1&type=pdf) Infancy (2004).

**M. Hayhoe**. [Vision and action.](https://www.annualreviews.org/doi/abs/10.1146/annurev-vision-102016-061437) Annual Review of Vision Science (2017).

**E. Kasneci**, G. Kasneci, T. Kübler, W. Rosenstiel. [Online recognition of fixations, saccades, and smooth pursuits for automated analysis of traffic hazard perception.](http://ti.uni-tuebingen.de/uploads/tx_timitarbeiter/Kasneci_et_al_Online_Recognition_Eye_Movements.pdf) Artificial neural networks (2015).

P. Kiefer, I. Giannopoulos, M. Raubal, **A. Duchowski**. [Eye tracking for spatial research: Cognition, computation, challenges.](https://n.ethz.ch/~pekiefer/download/papers/kiefer2017scc_preprint.pdf) Spatial Cognition & Computation (2017)

D. Kit, L. Katz, B. Sullivan, K. Snyder, D. Ballard, **M. Hayhoe**. [Eye movements, visual search and scene memory, in an immersive virtual environment.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0094362) PLoS One (2015).

G. Kütt, K. Lee, E. Hardacre, **A. Papoutsaki**. [Eye-write: Gaze sharing for collaborative writing.](https://dl.acm.org/citation.cfm?id=3300727) CHI (2019).

M. Land, **M. Hayhoe**. [In what ways do eye movements contribute to everyday activities?](https://www.sciencedirect.com/science/article/pii/S004269890100102X) Vision research (2001).

D. Lohr, S.-H. Berndt, **O. Komogortsev**. [An implementation of eye movement-driven biometrics in virtual reality.](https://par.nsf.gov/servlets/purl/10074023) ETRA (2018).

**L. Matzen**, M. Haass, K. Divis, M. Stites. [Patterns of attention: How data visualizations are read.](https://www.osti.gov/servlets/purl/1425321) International Conference on Augmented Cognition (2017).

**A. Papoutsaki**, J. Laskey, J. Huang. [Searchgazer: Webcam eye tracking for remote studies of web search.](https://par.nsf.gov/servlets/purl/10024075) CHIIR (2017).

K. Rayner, **M. Castelhano**, J. Yang. [Eye movements when looking at unusual/weird scenes: Are there cultural differences?](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2668126/). Journal of Experimental Psychology (2009).

**B. Sharif**, M. Falcone, J. Maletic. [An eye-tracking study on the role of scan time in finding source code defects.](http://www.cs.kent.edu/~jmaletic/papers/ETRA12.pdf) ETRA (2012).

A. Torralba, A. Oliva, **M. Castelhano**, J. Henderson. [Contextual guidance of eye movements and attention in real-world scenes: the role of global features in object search.](http://jhenderson.org/vclab/PDF_Pubs/Torralba_Oliva_Castelhano_Henderson_Psych_Review.pdf) Psychological review (2006).

O. Mercier, Y. Sulai, K. Mackenzie, M. Zannoli, **J. Hillis**, D. Nowrouzezahrai, D. Lanman. [Fast gaze-contingent optimal decompositions for multifocal displays.](https://dl.acm.org/citation.cfm?id=3130846) TOG (2017).

S. Vitak, J. Ingram, **A. Duchowski**, S. Ellis, A. Gramopadhye. [Gaze-augmented think-aloud as an aid to learning.](https://dl.acm.org/citation.cfm?id=2208710) CHI (2012).

K. Yun, Y. Peng, D. Samaras, **G. Zelinsky**, T. Berg. [Exploring the role of gaze behavior and object detection in scene understanding.](https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00917/full?utm_source=newsletter&utm_medium=web&utm_campaign=Psychology-w4-2014) Frontiers in psychology (2013)

**X. Wang**, A. Ley, S. Koch, D. Lindlbauer, J. Hays, K. Holmqvist, M. Alexa. [The mental image revealed by gaze tracking.](https://dl.acm.org/citation.cfm?id=3300839) CHI (2019).

**G. Zelinsky**, Y. Peng, D. Samaras. [Eye can read your mind: Decoding gaze fixations to reveal categorical search targets.](https://iovs.arvojournals.org/article.aspx?articleid=2193783) Journal of vision (2013).

## Partners
<img height="100" src="https://emics-2020.github.io/EMICS/assets/logos/Adobe.png">
<img height="100" src="https://emics-2020.github.io/EMICS/assets/logos/frl.png">

